{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.stats import norm\n",
    "\n",
    "# print(\"Python executable:\", sys.executable)\n",
    "# print(\"TensorFlow version:\", tf.__version__)\n",
    "# print(\"TensorFlow Probability version:\", tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd15b12-8c6c-4aa0-b6c0-308d0a1b942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
   
    "# Load dataset\n",
    
    "data = pd.read_csv(r\"C:\\Users\\Private\\Downloads\\Assignment 4 2025.csv\")\n",
    "#data.head(10)\n",
    "\n",
    "# Labels / features\n",
    "data['event'] = 1 - data['censor']\n",
    "numeric_cols = ['support_tickets', 'payment_delays', 'total_spend']\n",
    "categorical_cols = ['industry_type', 'supplier_engagement']\n",
    "\n",
    "X_numeric = data[numeric_cols].values\n",
    "X_categorical = pd.get_dummies(data[categorical_cols], drop_first=True)\n",
    "feature_names = numeric_cols + X_categorical.columns.tolist()\n",
    "X = np.hstack((X_numeric, X_categorical.values)).astype('float32')\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X).astype('float32')\n",
    "\n",
    "T = data['duration'].values.astype('float32')\n",
    "delta = data['event'].values.astype('float32')\n",
    "\n",
    "# Split into train/val/test (60%/20%/20%)\n",
    "X_temp, X_test, T_temp, T_test, delta_temp, delta_test = train_test_split(\n",
    "    X, T, delta, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, T_train, T_val, delta_train, delta_val = train_test_split(\n",
    "    X_temp, T_temp, delta_temp, test_size=0.25, random_state=42  # 0.25 of 80% = 20% val\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Val size: {len(X_val)}, Test size: {len(X_test)}\")\n",
    "\n",
    "# Convert to tf tensors for efficiency\n",
    "X_train_tf = tf.convert_to_tensor(X_train)\n",
    "X_val_tf = tf.convert_to_tensor(X_val)\n",
    "X_test_tf = tf.convert_to_tensor(X_test)\n",
    "y_train_tf = tf.convert_to_tensor(np.column_stack((T_train, delta_train)))\n",
    "y_val_tf = tf.convert_to_tensor(np.column_stack((T_val, delta_val)))\n",
    "y_test_tf = tf.convert_to_tensor(np.column_stack((T_test, delta_test)))\n",
    "\n",
    
    "# Mixture Density Model \n",
   
    "def build_density_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), dtype=tf.float32),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(15)\n",
    "    ])\n",
    "    return model\n",
    "\n",
 
    "# Shared model builder for eta(x)\n",
  
    "def build_eta_model(input_shape):\n",
    "    inputs = tf.keras.Input(shape=(input_shape,), dtype=tf.float32)\n",
    "    eta = tf.keras.layers.Dense(1, use_bias=False)(inputs)\n",
    "    return tf.keras.Model(inputs, eta)\n",
    "\n",
    "# Numeric stability constants\n",
    "EPS = tf.constant(1e-10, dtype=tf.float32)\n",
    "EPS_SIG = tf.constant(1e-6, dtype=tf.float32)\n",
    "\n",
    
    "# 1. Mixture AFT Model\n",

    "print(\"\\n--- Training Mixture AFT Model ---\")\n",
    "density_model = build_density_model()\n",
    "model_mixture = build_eta_model(X.shape[1])\n",
    "sigma_mixture = tf.Variable(tf.constant(1.0, dtype=tf.float32), trainable=True, name='sigma_mixture')\n",
    "\n",
    "# All trainable variables for mixture\n",
    "all_trainable_mixture = (model_mixture.trainable_variables + \n",
    "                         [sigma_mixture] + \n",
    "                         density_model.trainable_variables)\n",
    "\n",
    "optimizer_mixture = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "def aft_loss_mixture(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    T_obs = y_true[:, 0]\n",
    "    d_event = y_true[:, 1]\n",
    "    T_obs = tf.maximum(T_obs, EPS)\n",
    "    sig = tf.nn.softplus(sigma_mixture) + EPS\n",
    "    z_scalar = (tf.math.log(T_obs) - y_pred[:, 0]) / sig\n",
    "    z_scalar = tf.clip_by_value(z_scalar, -10.0, 10.0)  # Clip for stability\n",
    "    params = density_model(tf.expand_dims(z_scalar, -1))\n",
    "    pi_raw, mu, sigma_nn = tf.split(params, 3, axis=-1)\n",
    "    pi = tf.nn.softmax(pi_raw, axis=-1)\n",
    "    sigma_nn = tf.nn.softplus(sigma_nn) + EPS_SIG\n",
    "    cat = tfp.distributions.Categorical(probs=pi)\n",
    "    comps = tfp.distributions.Normal(loc=mu, scale=sigma_nn)\n",
    "    dist = tfp.distributions.MixtureSameFamily(mixture_distribution=cat, components_distribution=comps)\n",
    "    f_w = dist.prob(z_scalar)\n",
    "    S_w = dist.survival_function(z_scalar)\n",
    "    f_w = tf.maximum(f_w, EPS)\n",
    "    S_w = tf.maximum(S_w, EPS)\n",
    "    log_f = tf.math.log(f_w) - tf.math.log(sig) - tf.math.log(T_obs)\n",
    "    log_S = tf.math.log(S_w)\n",
    "    nll = - (d_event * log_f + (1.0 - d_event) * log_S)\n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "@tf.function\n",
    "def train_step_mixture(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        eta_batch = model_mixture(X_batch, training=True)\n",
    "        loss = aft_loss_mixture(y_batch, eta_batch)\n",
    "    grads = tape.gradient(loss, all_trainable_mixture)\n",
    "    optimizer_mixture.apply_gradients(zip(grads, all_trainable_mixture))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_mixture(X, y):\n",
    "    eta = model_mixture(X, training=False)\n",
    "    return aft_loss_mixture(y, eta)\n",
    "\n",
    "# Training loop\n",
    "batch_size = 32\n",
    "train_losses_mixture = []\n",
    "val_losses_mixture = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in range(0, len(X_train_tf), batch_size):\n",
    "        end_i = min(i + batch_size, len(X_train_tf))\n",
    "        X_batch = X_train_tf[i:end_i]\n",
    "        y_batch = y_train_tf[i:end_i]\n",
    "        loss = train_step_mixture(X_batch, y_batch)\n",
    "        epoch_train_loss += float(loss)\n",
    "        num_batches += 1\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    train_losses_mixture.append(avg_train_loss)\n",
    "    \n",
    "    val_loss = float(compute_loss_mixture(X_val_tf, y_val_tf))\n",
    "    val_losses_mixture.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Mixture Epoch {epoch+1}/50 - Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "# Test loss for mixture\n",
    "test_loss_mixture = float(compute_loss_mixture(X_test_tf, y_test_tf))\n",
    "print(f\"Mixture AFT Test NLL: {test_loss_mixture:.6f}\")\n",
    "\n",
    
    "# 2. Weibull AFT Model\n",
   
    "print(\"\\n--- Training Weibull AFT Model ---\")\n",
    "model_weibull = build_eta_model(X.shape[1])\n",
    "rho_weibull = tf.Variable(tf.constant(1.0, dtype=tf.float32), trainable=True, name='rho_weibull')\n",
    "\n",
    "# All trainable variables for Weibull\n",
    "all_trainable_weibull = model_weibull.trainable_variables + [rho_weibull]\n",
    "\n",
    "optimizer_weibull = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "\n",
    "def aft_loss_weibull(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    T_obs = y_true[:, 0]\n",
    "    d_event = y_true[:, 1]\n",
    "    T_obs = tf.maximum(T_obs, EPS)\n",
    "    rho = tf.nn.softplus(rho_weibull) + EPS\n",
    "    eta = y_pred[:, 0]\n",
    "    log_t = tf.math.log(T_obs)\n",
    "    z = log_t - eta\n",
    "    z = tf.clip_by_value(z, -20.0, 20.0)  # Clip for stability\n",
    "    term1 = tf.exp(rho * z)\n",
    "    log_f = tf.math.log(rho) + (rho - 1.0) * log_t - eta - term1\n",
    "    log_S = - term1\n",
    "    nll = - (d_event * log_f + (1.0 - d_event) * log_S)\n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "@tf.function\n",
    "def train_step_weibull(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        eta_batch = model_weibull(X_batch, training=True)\n",
    "        loss = aft_loss_weibull(y_batch, eta_batch)\n",
    "    grads = tape.gradient(loss, all_trainable_weibull)\n",
    "    optimizer_weibull.apply_gradients(zip(grads, all_trainable_weibull))\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_weibull(X, y):\n",
    "    eta = model_weibull(X, training=False)\n",
    "    return aft_loss_weibull(y, eta)\n",
    "\n",
    "# Training loop\n",
    "train_losses_weibull = []\n",
    "val_losses_weibull = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i in range(0, len(X_train_tf), batch_size):\n",
    "        end_i = min(i + batch_size, len(X_train_tf))\n",
    "        X_batch = X_train_tf[i:end_i]\n",
    "        y_batch = y_train_tf[i:end_i]\n",
    "        loss = train_step_weibull(X_batch, y_batch)\n",
    "        epoch_train_loss += float(loss)\n",
    "        num_batches += 1\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    train_losses_weibull.append(avg_train_loss)\n",
    "    \n",
    "    val_loss = float(compute_loss_weibull(X_val_tf, y_val_tf))\n",
    "    val_losses_weibull.append(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Weibull Epoch {epoch+1}/50 - Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "# Test loss for Weibull\n",
    "test_loss_weibull = float(compute_loss_weibull(X_test_tf, y_test_tf))\n",
    "print(f\"Weibull AFT Test NLL: {test_loss_weibull:.6f}\")\n",
    "\n",
    
    "# Function to compute stds using numerical Hessian (with fixes for stability)\n",
    
    "def compute_stds(model, scale_var, loss_fn, X_data, y_data):\n",
    "    n = X_data.shape[0]\n",
    "    # Save current\n",
    "    beta_save = model.layers[1].kernel.numpy()\n",
    "    scale_save = scale_var.numpy()\n",
    "    beta_current = beta_save.flatten()\n",
    "    scale_current = scale_save\n",
    "    theta_current = np.concatenate([beta_current, [scale_current]])\n",
    "    num_params = len(theta_current)\n",
    "    beta_shape = model.layers[1].kernel.shape  # e.g., (9, 1)\n",
    "\n",
    "    def nll_func(theta):\n",
    "        beta_new = theta[:-1].reshape(beta_shape).astype(np.float32)\n",
    "        scale_new = np.array(theta[-1], dtype=np.float32)\n",
    "        model.layers[1].kernel.assign(beta_new)\n",
    "        scale_var.assign(scale_new)\n",
    "        eta = model(X_data, training=False)\n",
    "        loss_value = loss_fn(y_data, eta)\n",
    "        # Handle potential inf/nan\n",
    "        loss_value = tf.where(tf.math.is_finite(loss_value), loss_value, tf.constant(10000.0, dtype=tf.float32))\n",
    "        return float(loss_value.numpy()) if hasattr(loss_value, 'numpy') else float(loss_value)\n",
    "\n",
    "    # Numerical Hessian of mean NLL\n",
    "    eps = 1e-6  # Smaller eps for better accuracy\n",
    "    hessian = np.zeros((num_params, num_params))\n",
    "    for j in range(num_params):\n",
    "        theta_plus_j = theta_current.copy()\n",
    "        theta_plus_j[j] += eps\n",
    "        score_plus = np.zeros(num_params)\n",
    "        for k in range(num_params):\n",
    "            theta_pk = theta_plus_j.copy()\n",
    "            theta_pk[k] += eps\n",
    "            nll_pk = nll_func(theta_pk)\n",
    "            theta_pk[k] -= 2 * eps\n",
    "            nll_pm = nll_func(theta_pk)\n",
    "            score_plus[k] = (nll_pk - nll_pm) / (2 * eps)\n",
    "        theta_minus_j = theta_current.copy()\n",
    "        theta_minus_j[j] -= eps\n",
    "        score_minus = np.zeros(num_params)\n",
    "        for k in range(num_params):\n",
    "            theta_mk = theta_minus_j.copy()\n",
    "            theta_mk[k] += eps\n",
    "            nll_mk = nll_func(theta_mk)\n",
    "            theta_mk[k] -= 2 * eps\n",
    "            nll_mm = nll_func(theta_mk)\n",
    "            score_minus[k] = (nll_mk - nll_mm) / (2 * eps)\n",
    "        for k in range(num_params):\n",
    "            hessian[k, j] = (score_plus[k] - score_minus[k]) / (2 * eps)\n",
    "\n",
    "    # Ensure hessian is symmetric and positive semi-definite\n",
    "    hessian = (hessian + hessian.T) / 2  # Symmetrize\n",
    "    eigvals, _ = np.linalg.eigh(hessian)\n",
    "    hessian = np.maximum(hessian, 0)  # Clip negative parts roughly\n",
    "\n",
    "    # Covariance matrix with pinv and ridge\n",
    "    ridge = 1e-5 * np.trace(np.abs(hessian)) / num_params * np.eye(num_params)\n",
    "    info_matrix = n * (hessian + ridge)\n",
    "    cov_matrix = np.linalg.pinv(info_matrix)\n",
    "\n",
    "    # Clip diag to non-negative\n",
    "    diag_cov = np.diag(cov_matrix)\n",
    "    diag_cov = np.maximum(diag_cov, 0)\n",
    "    stds = np.sqrt(diag_cov)\n",
    "\n",
    "    # Restore parameters\n",
    "    model.layers[1].kernel.assign(beta_save)\n",
    "    scale_var.assign(scale_save)\n",
    "\n",
    "    return stds\n",
    "\n",
  
    "# Inference for Mixture AFT\n",
  
    "print(\"\\n--- Mixture AFT Inference ---\")\n",
    "stds_mixture = compute_stds(model_mixture, sigma_mixture, aft_loss_mixture, X_train_tf, y_train_tf)\n",
    "\n",
    "beta_mixture_flat = model_mixture.layers[1].kernel.numpy().flatten()\n",
    "raw_sigma_mixture = sigma_mixture.numpy()\n",
    "soft_sigma_mixture = float(tf.nn.softplus(raw_sigma_mixture))\n",
    "stds_beta_mixture = stds_mixture[:-1]\n",
    "std_raw_sigma_mixture = stds_mixture[-1]\n",
    "deriv_softplus = float(tf.nn.sigmoid(raw_sigma_mixture))\n",
    "std_soft_sigma_mixture = std_raw_sigma_mixture * deriv_softplus\n",
    "\n",
    "print(\"Mixture AFT Estimated beta coefficients:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    coef = beta_mixture_flat[i]\n",
    "    se = stds_beta_mixture[i]\n",
    "    if se <= 0:\n",
    "        z = np.inf if coef > 0 else -np.inf\n",
    "        p = 0.0\n",
    "    else:\n",
    "        z = coef / se\n",
    "        p = 2 * (1 - norm.cdf(np.abs(z)))\n",
    "    low = coef - 1.96 * se\n",
    "    high = coef + 1.96 * se\n",
    "    print(f\"{name}: {coef:.6f} (SE={se:.6f}, z={z:.6f}, p={p:.6f}, 95% CI [{low:.6f}, {high:.6f}])\")\n",
    "\n",
    "if std_soft_sigma_mixture <= 0:\n",
    "    z_sig = np.inf\n",
    "    p_sig = 0.0\n",
    "else:\n",
    "    z_sig = soft_sigma_mixture / std_soft_sigma_mixture\n",
    "    p_sig = 2 * (1 - norm.cdf(np.abs(z_sig)))\n",
    "low_sig = soft_sigma_mixture - 1.96 * std_soft_sigma_mixture\n",
    "high_sig = soft_sigma_mixture + 1.96 * std_soft_sigma_mixture\n",
    "print(f\"Mixture AFT Estimated sigma (softplus): {soft_sigma_mixture:.6f} (SE={std_soft_sigma_mixture:.6f}, z={z_sig:.6f}, p={p_sig:.6f}, 95% CI [{low_sig:.6f}, {high_sig:.6f}])\")\n",
    "\n",
 
    "# Inference for Weibull AFT\n",
   
    "print(\"\\n--- Weibull AFT Inference ---\")\n",
    "stds_weibull = compute_stds(model_weibull, rho_weibull, aft_loss_weibull, X_train_tf, y_train_tf)\n",
    "\n",
    "beta_weibull_flat = model_weibull.layers[1].kernel.numpy().flatten()\n",
    "raw_rho_weibull = rho_weibull.numpy()\n",
    "soft_rho_weibull = float(tf.nn.softplus(raw_rho_weibull))\n",
    "stds_beta_weibull = stds_weibull[:-1]\n",
    "std_raw_rho_weibull = stds_weibull[-1]\n",
    "deriv_softplus_rho = float(tf.nn.sigmoid(raw_rho_weibull))\n",
    "std_soft_rho_weibull = std_raw_rho_weibull * deriv_softplus_rho\n",
    "\n",
    "print(\"Weibull AFT Estimated beta coefficients:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    coef = beta_weibull_flat[i]\n",
    "    se = stds_beta_weibull[i]\n",
    "    if se <= 0:\n",
    "        z = np.inf if coef > 0 else -np.inf\n",
    "        p = 0.0\n",
    "    else:\n",
    "        z = coef / se\n",
    "        p = 2 * (1 - norm.cdf(np.abs(z)))\n",
    "    low = coef - 1.96 * se\n",
    "    high = coef + 1.96 * se\n",
    "    print(f\"{name}: {coef:.6f} (SE={se:.6f}, z={z:.6f}, p={p:.6f}, 95% CI [{low:.6f}, {high:.6f}])\")\n",
    "\n",
    "if std_soft_rho_weibull <= 0:\n",
    "    z_rho = np.inf\n",
    "    p_rho = 0.0\n",
    "else:\n",
    "    z_rho = soft_rho_weibull / std_soft_rho_weibull\n",
    "    p_rho = 2 * (1 - norm.cdf(np.abs(z_rho)))\n",
    "low_rho = soft_rho_weibull - 1.96 * std_soft_rho_weibull\n",
    "high_rho = soft_rho_weibull + 1.96 * std_soft_rho_weibull\n",
    "print(f\"Weibull AFT Estimated rho (softplus): {soft_rho_weibull:.6f} (SE={std_soft_rho_weibull:.6f}, z={z_rho:.6f}, p={p_rho:.6f}, 95% CI [{low_rho:.6f}, {high_rho:.6f}])\")\n",
    "\n",
    
    "# Comparison\n",
 
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(f\"Mixture AFT Test NLL: {test_loss_mixture:.6f}\")\n",
    "print(f\"Weibull AFT Test NLL: {test_loss_weibull:.6f}\")\n",
    "if test_loss_mixture < test_loss_weibull:\n",
    "    print(\"Mixture AFT has lower test NLL (better fit).\")\n",
    "else:\n",
    "    print(\"Weibull AFT has lower test NLL (better fit).\")\n",
    "\n",
   
    "# Plot training curves for both\n",
    
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(train_losses_mixture, label='Training Loss')\n",
    "ax1.plot(val_losses_mixture, label='Validation Loss')\n",
    "ax1.set_title('Mixture AFT Training Curves')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Negative Log-Likelihood')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_losses_weibull, label='Training Loss')\n",
    "ax2.plot(val_losses_weibull, label='Validation Loss')\n",
    "ax2.set_title('Weibull AFT Training Curves')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Negative Log-Likelihood')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    
    "# Predict median survival time (approx) on test set\n",
    "# For mixture (symmetric zero-median baseline): median(T|x) ≈ exp(eta)\n",
    "# For Weibull: median(T|x) = exp(eta / rho) * (log(2))^{1/rho}\n",
   
    "eta_test_mixture = model_mixture.predict(X_test, verbose=0).astype('float32').squeeze()\n",
    "T_pred_median_mixture = np.exp(eta_test_mixture)\n",
    "\n",
    "rho_val = soft_rho_weibull\n",
    "eta_test_weibull = model_weibull.predict(X_test, verbose=0).astype('float32').squeeze()\n",
    "scale_factor = np.log(2) ** (1.0 / rho_val)\n",
    "T_pred_median_weibull = np.exp(eta_test_weibull / rho_val) * scale_factor\n",
    "\n",
    "print(f\"\\nTest set median survival predictions:\")\n",
    "print(f\"Mixture AFT - Mean: {np.mean(T_pred_median_mixture):.6f}, Median: {np.median(T_pred_median_mixture):.6f}\")\n",
    "print(f\"Weibull AFT - Mean: {np.mean(T_pred_median_weibull):.6f}, Median: {np.median(T_pred_median_weibull):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175751e",
   "metadata": {},
   "source": [
    "**HARELLS'S C TEST**\n",
    "\n",
    "C-index measures discrimination. The Mixture AFT has better(higher) discrimination $[0.5, 0.688]$ than the WEIBULL AFT $[0.45, 0.66]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIXTURE AFT\n",
    "# Bootstrap 95% CI for C-index (Harrell's C) on Test\n",
    "\n",
    "# Assuming c_index is defined from previous code (e.g., concordance index function)\n",
    "# If not, define it here or import from lifelines.utils or similar\n",
    "from lifelines.utils import concordance_index  # Add this if needed\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "B = 300\n",
    "c_vals = []\n",
    "for _ in range(B):\n",
    "    idx = rng.integers(0, len(T_test), size=len(T_test))\n",
    "    c_vals.append(concordance_index(T_test[idx], -eta_test_mixture[idx], delta_test[idx]))\n",
    "c_vals = np.array(c_vals)\n",
    "c_lo, c_hi = np.percentile(c_vals, [2.5, 97.5])\n",
    "print(f\"C-index 95% CI (bootstrap): [{c_lo:.3f}, {c_hi:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIBULL AFT\n",
    "#  Bootstrap 95% CI for C-index (Harrell's C) on Test\n",
    "\n",
 
    "from lifelines.utils import concordance_index  \n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "B_W = 300\n",
    "c_vals_W = []\n",
    "for _ in range(B_W):\n",
    "    idx = rng.integers(0, len(T_test), size=len(T_test))\n",
    "    c_vals_W.append(concordance_index(T_test[idx], -eta_test_weibull[idx], delta_test[idx]))\n",
    "c_vals = np.array(c_vals_W)\n",
    "c_lo_W, c_hi_W = np.percentile(c_vals_W, [2.5, 97.5])\n",
    "print(f\"C-index 95% CI WEIBULL (bootstrap): [{c_lo_W:.3f}, {c_hi_W:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb9f19",
   "metadata": {},
   "source": [
    "HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a346f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eps = tf.constant(1e-10, tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def mixture_params_of_z(z_scalar):\n",
    "    params = density_model(tf.expand_dims(z_scalar, -1))  # (batch, 15)\n",
    "    pi_raw, mu, sigK = tf.split(params, 3, axis=-1)       # each (batch, K)\n",
    "    pi   = tf.nn.softmax(pi_raw, axis=-1)\n",
    "    sigK = tf.nn.softplus(sigK) + tf.constant(1e-6, tf.float32)\n",
    "    return pi, mu, sigK\n",
    "\n",
    "@tf.function\n",
    "def survival_Z(z_scalar):\n",
    "    pi, mu, sigK = mixture_params_of_z(z_scalar)\n",
    "    cat  = tfp.distributions.Categorical(probs=pi)\n",
    "    comps= tfp.distributions.Normal(loc=mu, scale=sigK)\n",
    "    dist = tfp.distributions.MixtureSameFamily(mixture_distribution=cat,\n",
    "                                               components_distribution=comps)\n",
    "    return dist.survival_function(z_scalar)\n",
    "\n",
    "@tf.function\n",
    "def cdf_Z(z_scalar):\n",
    "    pi, mu, sigK = mixture_params_of_z(z_scalar)\n",
    "    cat  = tfp.distributions.Categorical(probs=pi)\n",
    "    comps= tfp.distributions.Normal(loc=mu, scale=sigK)\n",
    "    dist = tfp.distributions.MixtureSameFamily(mixture_distribution=cat,\n",
    "                                               components_distribution=comps)\n",
    "    return dist.cdf(z_scalar)\n",
    "\n",
    "def S_T_vec(t_vec, eta_vec, sig_scalar):\n",
    "    t_tf   = tf.constant(t_vec.astype('float32'))\n",
    "    eta_tf = tf.constant(eta_vec.astype('float32'))\n",
    "    sig_tf = tf.constant(sig_scalar, tf.float32)\n",
    "    Z = (tf.math.log(t_tf)[tf.newaxis,:] - eta_tf[:,tf.newaxis]) / sig_tf\n",
    "    S = survival_Z(tf.reshape(Z, [-1]))\n",
    "    return tf.reshape(S, [tf.shape(eta_tf)[0], tf.shape(t_tf)[0]]).numpy()\n",
    "\n",
    "def F_T_vec(t_vec, eta_vec, sig_scalar):\n",
    "    t_tf   = tf.constant(t_vec.astype('float32'))\n",
    "    eta_tf = tf.constant(eta_vec.astype('float32'))\n",
    "    sig_tf = tf.constant(sig_scalar, tf.float32)\n",
    "    Z = (tf.math.log(t_tf)[tf.newaxis,:] - eta_tf[:,tf.newaxis]) / sig_tf\n",
    "    F = cdf_Z(tf.reshape(Z, [-1]))\n",
    "    return tf.reshape(F, [tf.shape(eta_tf)[0], tf.shape(t_tf)[0]]).numpy()\n",
    "\n",
    "eta_test = model_mixture.predict(X_test, verbose=0).squeeze()\n",
    "sig_hat  = float(tf.nn.softplus(sigma_mixture).numpy())\n",
    "\n",
    "# Time grid (focus on central region)\n",
    "t_grid = np.linspace(np.percentile(T_test, 5), np.percentile(T_test, 95), 120)\n",
    "S_pred = S_T_vec(t_grid, eta_test_mixture, sigma_mixture)                # shape (n_test, M)\n",
    "F_pred = 1.0 - S_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0821c5",
   "metadata": {},
   "source": [
    "**DYNAMIC AUC CURVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb79c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYBRID AFT\n",
    "def dynamic_auc_at_time(t0, T, d, risk):\n",
    "    # Comparable pairs: one event before t0 vs one still at risk at t0\n",
    "    case = (T <= t0) & (d == 1)\n",
    "    ctrl = (T > t0)\n",
    "    if case.sum()==0 or ctrl.sum()==0:\n",
    "        return np.nan\n",
    "    r_case = risk[case][:,None]\n",
    "    r_ctrl = risk[ctrl][None,:]\n",
    "    cmp = (r_case > r_ctrl).mean() + 0.5*(r_case == r_ctrl).mean()\n",
    "    return float(cmp)\n",
    "\n",
    
    "# (12) Time-dependent AUC curve (dense horizons)\n",
 
    "dense_times = np.linspace(np.percentile(T_test[delta_test==1], 10),\n",
    "                          np.percentile(T_test[delta_test==1], 90), 15)\n",
    "auc_curve = [dynamic_auc_at_time(t0, T_test, delta_test, -eta_test_mixture) for t0 in dense_times]\n",
    "plt.figure()\n",
    "plt.plot(dense_times, auc_curve, '-o')\n",
    "plt.xlabel(\"Time horizon t\")\n",
    "plt.ylabel(\"Dynamic AUC\")\n",
    "plt.title(\"Dynamic AUC across time\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "print(f\"Dynamic AUC curve plot saved to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEIBULL AFT\n",
    "def dynamic_auc_at_time(t0, T, d, risk):\n",
    "    # Comparable pairs: one event before t0 vs one still at risk at t0\n",
    "    case = (T <= t0) & (d == 1)\n",
    "    ctrl = (T > t0)\n",
    "    if case.sum()==0 or ctrl.sum()==0:\n",
    "        return np.nan\n",
    "    r_case = risk[case][:,None]\n",
    "    r_ctrl = risk[ctrl][None,:]\n",
    "    cmp = (r_case > r_ctrl).mean() + 0.5*(r_case == r_ctrl).mean()\n",
    "    return float(cmp)\n",
    "\n",
    
    "# (12) Time-dependent AUC curve (dense horizons)\n",
  
    "dense_times_w = np.linspace(np.percentile(T_test[delta_test==1], 10),\n",
    "                          np.percentile(T_test[delta_test==1], 90), 15)\n",
    "auc_curve_w = [dynamic_auc_at_time(t0, T_test, delta_test, -eta_test_weibull) for t0 in dense_times]\n",
    "plt.figure()\n",
    "plt.plot(dense_times_w, auc_curve_w, '-o')\n",
    "plt.xlabel(\"Time horizon t\")\n",
    "plt.ylabel(\"Dynamic AUC\")\n",
    "plt.title(\"WEIBULL Dynamic AUC across time\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "print(f\"Dynamic AUC curve plot saved to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e548a",
   "metadata": {},
   "source": [
    "COX SNELL RESIDUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb967c77",
   "metadata": {},
   "outputs": [],
   "source": [
 
    "# 4) Cox–Snell residuals r_i = -log S(T_i|x_i)\n",
   
    "idx_T = np.searchsorted(t_grid, np.clip(T_test, t_grid[0], t_grid[-1]), side='left')\n",
    "S_obs = S_pred[np.arange(len(T_test)), np.clip(idx_T, 0, len(t_grid)-1)]\n",
    "r_cs  = -np.log(np.clip(S_obs, 1e-10, 1.0))\n",
    "# KM of r_cs should follow exp(-r) if well-specified\n",
    "r_grid = np.linspace(np.percentile(r_cs, 1), np.percentile(r_cs, 99), 120)\n",
    "def km_curve(times, events, grid):\n",
    "    order = np.argsort(times)\n",
    "    t = times[order]; e = events[order].astype(bool)\n",
    "    at_risk = len(t); S = 1.0; out=[]\n",
    "    j=0\n",
    "    for tg in grid:\n",
    "        while j < len(t) and t[j] <= tg:\n",
    "            S *= (at_risk - (1 if e[j] else 0)) / at_risk\n",
    "            at_risk -= 1; j += 1\n",
    "        out.append(S)\n",
    "    return np.array(out)\n",
    "\n",
    "S_cs = km_curve(r_cs, delta_test, r_grid)\n",
    "plt.figure()\n",
    "plt.plot(r_grid, S_cs, label=\"KM of Cox–Snell\")\n",
    "plt.plot(r_grid, np.exp(-r_grid), '--', label=\"Exp(-r) ideal\")\n",
    "plt.xlabel(\"r (Cox–Snell)\"); plt.ylabel(\"S(r)\")\n",
    "plt.title(\"Cox–Snell residual check\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "\n",
    "print(f\"Cox-Snell residuals plot saved to: {save_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd233a3",
   "metadata": {},
   "source": [
    "DECISION CURVE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc2020",
   "metadata": {},
   "outputs": [],
   "source": [
   
    "# (13) Decision Curve Analysis (Net Benefit) at horizon t*\n",
    "#      Net Benefit = TP/N - FP/N * (pt/(1-pt))\n",
    "#      Predicted event prob by t*: p = 1 - S(t*|x)\n",
  
    "t_star = np.percentile(T_test[delta_test==1], 60) if (delta_test==1).sum()>=5 else np.median(T_test)\n",
    "p_pred = 1.0 - S_T_vec(np.array([t_star]), eta_test_mixture, sigma_mixture)[:,0]  # per-sample prob of event by t*\n",
    "pt_grid = np.linspace(0.05, 0.5, 10)\n",
    "NB_model, NB_all, NB_none = [], [], []\n",
    "N = len(T_test)\n",
    "y_event_by_t = ((T_test <= t_star) & (delta_test==1)).astype(float)\n",
    "rate = y_event_by_t.mean()\n",
    "for pt in pt_grid:\n",
    "    treat = (p_pred >= pt).astype(float)\n",
    "    TP = ((treat==1) & (y_event_by_t==1)).sum()\n",
    "    FP = ((treat==1) & (y_event_by_t==0)).sum()\n",
    "    NB_model.append(TP/N - FP/N * (pt/(1-pt)))\n",
    "    # Treat-all and treat-none references\n",
    "    NB_all.append(rate - (1-rate) * (pt/(1-pt)))\n",
    "    NB_none.append(0.0)\n",
    "plt.figure()\n",
    "plt.plot(pt_grid, NB_model, '-o', label='Model')\n",
    "plt.plot(pt_grid, NB_all, '--', label='Treat-all')\n",
    "plt.plot(pt_grid, NB_none, ':', label='Treat-none')\n",
    "plt.xlabel(\"Threshold probability pt\")\n",
    "plt.ylabel(\"Net Benefit\")\n",
    "plt.title(f\"Decision Curve at t*={t_star:.1f}\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "\n",
    "print(f\"Decision curve analysis plot saved to: {save_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1734f8",
   "metadata": {},
   "source": [
    "PERMUTATION IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
 
    "# 7) Permutation importance (Δ Test NLL)\n",
  
    "def dataset_nll(XA, TA, dA, batch=1024):\n",
    "    n = XA.shape[0]; tot = 0.0\n",
    "    for i in range(0, n, batch):\n",
    "        sl = slice(i, min(i+batch, n))\n",
    "        yA = np.column_stack((TA[sl], dA[sl])).astype('float32')\n",
    "        y_pred = model_mixture(XA[sl], training=False)\n",
    "        tot += float(aft_loss_mixture(yA, y_pred).numpy()) * (sl.stop - sl.start)\n",
    "    return tot / n\n",
    "\n",
    "base_nll = dataset_nll(X_test, T_test, delta_test)\n",
    "impacts = []\n",
    "rng = np.random.default_rng(42)\n",
    "for j, name in enumerate(feature_names):\n",
    "    Xp = X_test.copy()\n",
    "    rng.shuffle(Xp[:, j])\n",
    "    imp = dataset_nll(Xp, T_test, delta_test) - base_nll\n",
    "    impacts.append((name, imp))\n",
    "impacts.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nPermutation importance (Δ Test NLL, higher = more important):\")\n",
    "for name, imp in impacts:\n",
    "    print(f\"{name:30s} {imp:+.6f}\")\n",
    "\n",
    "# Bar graph for permutation importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "names, imps = zip(*impacts)\n",
    "y_pos = np.arange(len(names))\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(names)))  # Gradient blue\n",
    "bars = plt.barh(y_pos, imps, color=colors, edgecolor='navy', alpha=0.8)\n",
    "plt.yticks(y_pos, names)\n",
    "plt.xlabel('Δ Test NLL (higher = more important)', fontsize=12)\n",
    "plt.title('Permutation Feature Importance', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Highest on top\n",
    "# Add value labels on bars\n",
    "for i, (bar, imp) in enumerate(zip(bars, imps)):\n",
    "    plt.text(bar.get_width() + max(0, imp)*0.01, bar.get_y() + bar.get_height()/2, f'{imp:.4f}', \n",
    "             ha='left' if imp > 0 else 'right', va='center', fontsize=10)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "print(f\"Permutation importance plot saved to: {save_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a65bbd",
   "metadata": {},
   "source": [
    "PARTIAL DEPENDENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19aee2",
   "metadata": {},
   "outputs": [],
   "source": [

    "# 6) Interpretability: Partial Dependence (feature effects)\n",
    "#    Vary one feature, keep others at their dataset mean (test set), and plot median(T|x)\n",

    "def pdp_numeric_feature(X_ref, feat_idx, grid, use_exp_eta=True):\n",
    "    \"\"\"\n",
    "    X_ref: reference matrix (N,D) to compute an average profile (mean of others)\n",
    "    feat_idx: index of numeric feature in X\n",
    "    grid: values to evaluate (already on standardized scale if X is standardized)\n",
    "    use_exp_eta: median(T|x) ≈ exp(eta) under zero-median baseline\n",
    "    returns grid, effect\n",
    "    \"\"\"\n",
    "    x_bar = X_ref.mean(axis=0, keepdims=True)  # (1,D)\n",
    "    X_eval = np.repeat(x_bar, len(grid), axis=0)\n",
    "    X_eval[:, feat_idx] = grid\n",
    "    eta_eval = model_mixture.predict(X_eval, verbose=0).squeeze()\n",
    "    if use_exp_eta:\n",
    "        return grid, np.exp(eta_eval)\n",
    "    else:\n",
    "        # Full numerical median via 0.5 survival is possible, but slower.\n",
    "        # You could replace with a root-finder using survival_T_at_times.\n",
    "        return grid, np.exp(eta_eval)\n",
    "\n",
    "# Build grids in standardized space for your 3 numeric features\n",
    "num_idx = [feature_names.index(c) for c in ['support_tickets','payment_delays','total_spend']]\n",
    "grids = []\n",
    "for i in num_idx:\n",
    "    v = X_test[:, i]\n",
    "    g = np.linspace(np.percentile(v,5), np.percentile(v,95), 40)\n",
    "    grids.append(g)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, (fi, g) in enumerate(zip(num_idx, grids), 1):\n",
    "    gx, gy = pdp_numeric_feature(X_test, fi, g)\n",
    "    plt.plot(gx, gy, label=f\"PDP: {feature_names[fi]}\")\n",
    "plt.xlabel(\"Standardized feature value\")\n",
    "plt.ylabel(\"Predicted median(T|x) ≈ exp(η)\")\n",
    "plt.title(\"Partial Dependence (median time) for numeric features\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
